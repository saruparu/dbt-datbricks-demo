# Databricks notebook source

# MAGIC %md
# MAGIC # üéõÔ∏è Databricks Control Flow ‚Äî Workflow Orchestration
# MAGIC
# MAGIC This notebook demonstrates how **control flow** works in Databricks using **Workflows** (Jobs API).
# MAGIC We define multi-task pipelines with dependencies, conditional execution, loops, and error handling.
# MAGIC
# MAGIC **This notebook is a reference guide** ‚Äî it defines workflow JSON payloads that can be submitted
# MAGIC to the Databricks Jobs API. You can also create these visually via the **Workflows UI**.

# COMMAND ----------

# MAGIC %md
# MAGIC ## 1Ô∏è‚É£ Workflow Overview
# MAGIC
# MAGIC A **Databricks Workflow** (Job) consists of:
# MAGIC
# MAGIC | Component | Description |
# MAGIC |-----------|-------------|
# MAGIC | **Job** | A named collection of tasks with a schedule |
# MAGIC | **Task** | A single unit of work (notebook, SQL, dbt, JAR, Python) |
# MAGIC | **Dependencies** | `depends_on` links between tasks (DAG) |
# MAGIC | **Trigger** | Schedule (cron), file arrival, or manual |
# MAGIC | **Parameters** | Key-value pairs passed between tasks |
# MAGIC | **Clusters** | Compute resources for each task |

# COMMAND ----------

# MAGIC %md
# MAGIC ## 2Ô∏è‚É£ Multi-Task Workflow ‚Äî dbt Pipeline
# MAGIC
# MAGIC This defines a complete dbt pipeline as a Databricks Workflow with 4 chained tasks:
# MAGIC
# MAGIC ```
# MAGIC dbt_seed ‚Üí dbt_run ‚Üí dbt_test ‚Üí generate_docs
# MAGIC ```

# COMMAND ----------

import json

# Define a multi-task dbt pipeline workflow
dbt_pipeline_workflow = {
    "name": "iot_dbt_pipeline",
    "description": "IoT Smart Factory ‚Äî dbt Bronze ‚Üí Silver ‚Üí Gold pipeline",

    # --- Schedule: Run every day at 6 AM UTC ---
    "schedule": {
        "quartz_cron_expression": "0 0 6 * * ?",
        "timezone_id": "UTC",
        "pause_status": "PAUSED"
    },

    # --- Email notifications on failure ---
    "email_notifications": {
        "on_failure": ["team@company.com"]
    },

    # --- Task definitions ---
    "tasks": [
        {
            "task_key": "dbt_seed",
            "description": "Load seed data (CSV ‚Üí Delta tables)",
            "dbt_task": {
                "project_directory": "/Workspace/Repos/iot-dbt-databricks",
                "commands": ["dbt seed --full-refresh"],
                "schema": "iot_dev",
                "warehouse_id": "0b4eee1bcc7b2623"
            },
            "timeout_seconds": 600
        },
        {
            "task_key": "dbt_run_bronze_silver",
            "description": "Build Bronze and Silver layer models",
            "depends_on": [{"task_key": "dbt_seed"}],
            "dbt_task": {
                "project_directory": "/Workspace/Repos/iot-dbt-databricks",
                "commands": [
                    "dbt run --select tag:bronze tag:silver"
                ],
                "schema": "iot_dev",
                "warehouse_id": "0b4eee1bcc7b2623"
            },
            "timeout_seconds": 1200
        },
        {
            "task_key": "dbt_test_silver",
            "description": "Run data quality tests on Silver layer",
            "depends_on": [{"task_key": "dbt_run_bronze_silver"}],
            "dbt_task": {
                "project_directory": "/Workspace/Repos/iot-dbt-databricks",
                "commands": ["dbt test --select tag:silver"],
                "schema": "iot_dev",
                "warehouse_id": "0b4eee1bcc7b2623"
            },
            "timeout_seconds": 600
        },
        {
            "task_key": "dbt_run_gold",
            "description": "Build Gold layer models (only if Silver tests pass)",
            "depends_on": [{"task_key": "dbt_test_silver"}],
            "dbt_task": {
                "project_directory": "/Workspace/Repos/iot-dbt-databricks",
                "commands": ["dbt run --select tag:gold"],
                "schema": "iot_dev",
                "warehouse_id": "0b4eee1bcc7b2623"
            },
            "timeout_seconds": 1200
        }
    ],

    # --- Max concurrent runs ---
    "max_concurrent_runs": 1
}

print("‚úÖ Multi-Task dbt Pipeline Workflow Definition:")
print(json.dumps(dbt_pipeline_workflow, indent=2))

# COMMAND ----------

# MAGIC %md
# MAGIC ### Task Dependency Graph
# MAGIC
# MAGIC ```
# MAGIC ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
# MAGIC ‚îÇ dbt_seed ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ dbt_run_bronze_    ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ dbt_test_      ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ dbt_run_gold ‚îÇ
# MAGIC ‚îÇ          ‚îÇ    ‚îÇ silver             ‚îÇ    ‚îÇ silver         ‚îÇ    ‚îÇ              ‚îÇ
# MAGIC ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
# MAGIC                                                 ‚îÇ
# MAGIC                                                 ‚îÇ (if tests fail)
# MAGIC                                                 ‚ñº
# MAGIC                                          ‚ùå Pipeline stops
# MAGIC                                          Gold is NOT built
# MAGIC ```
# MAGIC
# MAGIC üí° **This IS control flow** ‚Äî the Gold layer only runs if Silver tests pass. If `dbt_test_silver` fails, `dbt_run_gold` is automatically skipped.

# COMMAND ----------

# MAGIC %md
# MAGIC ## 3Ô∏è‚É£ Conditional Execution (IF/ELSE Tasks)
# MAGIC
# MAGIC Databricks Workflows support **condition tasks** that branch based on expressions.
# MAGIC This lets you create IF/ELSE logic in your pipeline.

# COMMAND ----------

# Workflow with conditional branching
conditional_workflow = {
    "name": "iot_conditional_pipeline",
    "description": "Pipeline with conditional Gold refresh based on anomaly count",

    "tasks": [
        # Task 1: Check anomaly count
        {
            "task_key": "check_anomaly_count",
            "description": "Count anomalies in Silver layer",
            "sql_task": {
                "query": {
                    "query_text": """
                        SELECT CASE 
                            WHEN count(*) > 50 THEN 'HIGH_ANOMALIES'
                            ELSE 'NORMAL'
                        END as anomaly_status
                        FROM workspace.iot_dev_silver.int_sensor_readings_cleaned
                        WHERE is_anomaly = true
                    """
                },
                "warehouse_id": "0b4eee1bcc7b2623"
            }
        },

        # Task 2: CONDITION ‚Äî Branch based on anomaly count
        {
            "task_key": "evaluate_anomalies",
            "description": "IF anomaly count > 50 ‚Üí run full refresh, ELSE ‚Üí normal incremental",
            "depends_on": [{"task_key": "check_anomaly_count"}],
            "condition_task": {
                "op": "EQUAL_TO",
                "left": "{{tasks.check_anomaly_count.values.anomaly_status}}",
                "right": "HIGH_ANOMALIES"
            }
        },

        # Task 3a: IF TRUE ‚Üí Full refresh Gold (recompute everything)
        {
            "task_key": "full_refresh_gold",
            "description": "Full refresh Gold tables due to high anomaly count",
            "depends_on": [
                {"task_key": "evaluate_anomalies", "outcome": "true"}
            ],
            "dbt_task": {
                "project_directory": "/Workspace/Repos/iot-dbt-databricks",
                "commands": ["dbt run --select tag:gold --full-refresh"],
                "schema": "iot_dev",
                "warehouse_id": "0b4eee1bcc7b2623"
            }
        },

        # Task 3b: IF FALSE ‚Üí Normal incremental run
        {
            "task_key": "incremental_gold",
            "description": "Normal incremental Gold update",
            "depends_on": [
                {"task_key": "evaluate_anomalies", "outcome": "false"}
            ],
            "dbt_task": {
                "project_directory": "/Workspace/Repos/iot-dbt-databricks",
                "commands": ["dbt run --select tag:gold"],
                "schema": "iot_dev",
                "warehouse_id": "0b4eee1bcc7b2623"
            }
        }
    ]
}

print("‚úÖ Conditional Workflow (IF/ELSE):")
print(json.dumps(conditional_workflow, indent=2))

# COMMAND ----------

# MAGIC %md
# MAGIC ### Conditional Flow
# MAGIC ```
# MAGIC ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
# MAGIC ‚îÇ check_anomaly_   ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ evaluate_anomalies   ‚îÇ
# MAGIC ‚îÇ count            ‚îÇ     ‚îÇ (CONDITION TASK)     ‚îÇ
# MAGIC ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
# MAGIC                                     ‚îÇ
# MAGIC                          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
# MAGIC                          ‚îÇ                     ‚îÇ
# MAGIC                     outcome=true          outcome=false
# MAGIC                          ‚îÇ                     ‚îÇ
# MAGIC                          ‚ñº                     ‚ñº
# MAGIC                 ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
# MAGIC                 ‚îÇ full_refresh_  ‚îÇ    ‚îÇ incremental_   ‚îÇ
# MAGIC                 ‚îÇ gold           ‚îÇ    ‚îÇ gold           ‚îÇ
# MAGIC                 ‚îÇ (--full-refresh‚îÇ    ‚îÇ (normal run)   ‚îÇ
# MAGIC                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
# MAGIC ```

# COMMAND ----------

# MAGIC %md
# MAGIC ## 4Ô∏è‚É£ ForEach Task ‚Äî Loop Over Plant Locations
# MAGIC
# MAGIC The **ForEach** task iterates over a list of values and runs a nested task for each.
# MAGIC Perfect for running the same logic per plant, per device type, or per date partition.

# COMMAND ----------

# Workflow with ForEach loop over plant locations
foreach_workflow = {
    "name": "iot_per_plant_analysis",
    "description": "Run device health analysis for each plant location",

    "tasks": [
        # Task 1: Get list of plant locations
        {
            "task_key": "get_plant_locations",
            "description": "Retrieve distinct plant locations",
            "sql_task": {
                "query": {
                    "query_text": """
                        SELECT DISTINCT plant_location 
                        FROM workspace.iot_dev_gold.dim_devices
                    """
                },
                "warehouse_id": "0b4eee1bcc7b2623"
            }
        },

        # Task 2: ForEach plant ‚Üí run analysis
        {
            "task_key": "analyze_per_plant",
            "description": "Run health analysis for each plant",
            "depends_on": [{"task_key": "get_plant_locations"}],
            "for_each_task": {
                "inputs": "{{tasks.get_plant_locations.values}}",
                "task": {
                    "task_key": "plant_health_check",
                    "sql_task": {
                        "query": {
                            "query_text": """
                                SELECT 
                                    '{{input}}' as plant,
                                    count(*) as total_readings,
                                    round(avg(health_score), 1) as avg_health,
                                    sum(CASE WHEN health_category = 'critical' THEN 1 ELSE 0 END) as critical_count
                                FROM workspace.iot_dev_gold.fct_device_summary s
                                JOIN workspace.iot_dev_gold.dim_devices d 
                                    ON s.device_id = d.device_id
                                WHERE d.plant_location = '{{input}}'
                            """
                        },
                        "warehouse_id": "0b4eee1bcc7b2623"
                    }
                },
                "concurrency": 3  # Run 3 plants in parallel
            }
        },

        # Task 3: Aggregate results after all plants complete
        {
            "task_key": "aggregate_results",
            "description": "Combine per-plant results into summary",
            "depends_on": [{"task_key": "analyze_per_plant"}],
            "notebook_task": {
                "notebook_path": "/Workspace/Repos/iot-dbt-databricks/notebooks/aggregate_health",
                "base_parameters": {
                    "run_date": "{{job.start_time.iso_date}}"
                }
            }
        }
    ]
}

print("‚úÖ ForEach Workflow (Loop Over Plants):")
print(json.dumps(foreach_workflow, indent=2))

# COMMAND ----------

# MAGIC %md
# MAGIC ### ForEach Flow
# MAGIC ```
# MAGIC ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
# MAGIC ‚îÇ get_plant_       ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ analyze_per_plant (ForEach)                ‚îÇ
# MAGIC ‚îÇ locations        ‚îÇ     ‚îÇ                                             ‚îÇ
# MAGIC ‚îÇ                  ‚îÇ     ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
# MAGIC ‚îÇ Returns:         ‚îÇ     ‚îÇ  ‚îÇ Detroit  ‚îÇ ‚îÇ Chicago  ‚îÇ ‚îÇ Austin   ‚îÇ   ‚îÇ
# MAGIC ‚îÇ [Detroit,        ‚îÇ     ‚îÇ  ‚îÇ health   ‚îÇ ‚îÇ health   ‚îÇ ‚îÇ health   ‚îÇ   ‚îÇ
# MAGIC ‚îÇ  Chicago,        ‚îÇ     ‚îÇ  ‚îÇ check    ‚îÇ ‚îÇ check    ‚îÇ ‚îÇ check    ‚îÇ   ‚îÇ
# MAGIC ‚îÇ  Austin]         ‚îÇ     ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
# MAGIC ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ       (runs in parallel, concurrency=3)    ‚îÇ
# MAGIC                          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
# MAGIC                                                ‚îÇ
# MAGIC                                                ‚ñº
# MAGIC                                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
# MAGIC                                    ‚îÇ aggregate_results    ‚îÇ
# MAGIC                                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
# MAGIC ```

# COMMAND ----------

# MAGIC %md
# MAGIC ## 5Ô∏è‚É£ Error Handling & Retry Policies
# MAGIC
# MAGIC Databricks Workflows support **automatic retries**, **timeouts**, and **failure notifications**.

# COMMAND ----------

# Task-level error handling configuration
error_handling_config = {
    "task_key": "dbt_run_with_retries",
    "description": "dbt run with robust error handling",

    "dbt_task": {
        "project_directory": "/Workspace/Repos/iot-dbt-databricks",
        "commands": ["dbt run --select tag:gold"],
        "schema": "iot_dev",
        "warehouse_id": "0b4eee1bcc7b2623"
    },

    # --- Retry Policy ---
    "retry_on_timeout": True,
    "max_retries": 3,             # Retry up to 3 times on failure
    "min_retry_interval_millis": 30000,   # Wait 30 seconds between retries
    "timeout_seconds": 1800,       # Timeout after 30 minutes

    # --- Notification on events ---
    "email_notifications": {
        "on_start": ["data-team@company.com"],
        "on_success": ["data-team@company.com"],
        "on_failure": ["oncall@company.com", "data-team@company.com"]
    },

    # --- Health rules (SLA monitoring) ---
    "health": {
        "rules": [
            {
                "metric": "RUN_DURATION_SECONDS",
                "op": "GREATER_THAN",
                "value": 900  # Alert if run takes > 15 minutes
            }
        ]
    }
}

print("‚úÖ Error Handling Configuration:")
print(json.dumps(error_handling_config, indent=2))

# COMMAND ----------

# MAGIC %md
# MAGIC ### Error Handling Summary
# MAGIC | Feature | Configuration | Purpose |
# MAGIC |---------|---------------|---------|
# MAGIC | **Retries** | `max_retries: 3` | Automatically retry failed tasks |
# MAGIC | **Retry delay** | `min_retry_interval_millis: 30000` | Wait 30s between retries |
# MAGIC | **Timeout** | `timeout_seconds: 1800` | Kill task if running > 30 mins |
# MAGIC | **Notifications** | `on_failure: [email]` | Alert on failure via email/Slack |
# MAGIC | **Health rules** | `RUN_DURATION > 900s` | SLA monitoring ‚Äî flag slow runs |

# COMMAND ----------

# MAGIC %md
# MAGIC ## 6Ô∏è‚É£ Parameters ‚Äî Passing Values Between Tasks
# MAGIC
# MAGIC Databricks Workflows support **job-level** and **task-level** parameters.
# MAGIC Tasks can reference outputs from upstream tasks.

# COMMAND ----------

# Parameterized workflow
parameterized_workflow = {
    "name": "iot_parameterized_pipeline",
    "description": "Pipeline with environment and threshold parameters",

    # Job-level parameters (can be overridden per run)
    "parameters": [
        {"name": "environment", "default": "dev"},
        {"name": "temperature_threshold", "default": "85.0"},
        {"name": "run_full_refresh", "default": "false"}
    ],

    "tasks": [
        {
            "task_key": "dbt_run_parameterized",
            "description": "Run dbt with dynamic parameters",
            "dbt_task": {
                "project_directory": "/Workspace/Repos/iot-dbt-databricks",
                "commands": [
                    # Use job parameters in dbt commands
                    "dbt run "
                    "--target {{job.parameters.environment}} "
                    "--vars '{temperature_upper: {{job.parameters.temperature_threshold}}}' "
                    + "{% if job.parameters.run_full_refresh == 'true' %}--full-refresh{% endif %}"
                ],
                "schema": "iot_{{job.parameters.environment}}",
                "warehouse_id": "0b4eee1bcc7b2623"
            }
        },
        {
            "task_key": "log_run_metadata",
            "description": "Log run metadata for audit trail",
            "depends_on": [{"task_key": "dbt_run_parameterized"}],
            "sql_task": {
                "query": {
                    "query_text": """
                        INSERT INTO workspace.iot_audit.run_log
                        VALUES (
                            current_timestamp(),
                            '{{job.parameters.environment}}',
                            '{{job.parameters.temperature_threshold}}',
                            '{{tasks.dbt_run_parameterized.result_state}}'
                        )
                    """
                },
                "warehouse_id": "0b4eee1bcc7b2623"
            }
        }
    ]
}

print("‚úÖ Parameterized Workflow:")
print(json.dumps(parameterized_workflow, indent=2))

# COMMAND ----------

# MAGIC %md
# MAGIC ### Parameter Reference Syntax
# MAGIC | Syntax | Resolves To |
# MAGIC |--------|-------------|
# MAGIC | `{{job.parameters.environment}}` | Job-level parameter value |
# MAGIC | `{{tasks.task_key.values.column}}` | Output value from a SQL task |
# MAGIC | `{{tasks.task_key.result_state}}` | Task result: SUCCESS, FAILED, etc. |
# MAGIC | `{{job.start_time.iso_date}}` | Job start date (2025-01-15) |
# MAGIC | `{{job.run_id}}` | Unique run ID |

# COMMAND ----------

# MAGIC %md
# MAGIC ## 7Ô∏è‚É£ Creating a Workflow via the Jobs API
# MAGIC
# MAGIC You can create workflows programmatically using the Databricks REST API.
# MAGIC Uncomment and run the cell below to create the multi-task dbt pipeline workflow.

# COMMAND ----------

# # Uncomment to create the workflow via API
# # Requires: DATABRICKS_HOST and DATABRICKS_TOKEN environment variables
#
# import requests
# import os
#
# host = os.environ.get("DATABRICKS_HOST", "https://dbc-d45fd83c-044d.cloud.databricks.com")
# token = os.environ.get("DATABRICKS_TOKEN", dbutils.secrets.get("dbt", "token"))
#
# response = requests.post(
#     f"{host}/api/2.1/jobs/create",
#     headers={"Authorization": f"Bearer {token}"},
#     json=dbt_pipeline_workflow
# )
#
# if response.status_code == 200:
#     job_id = response.json()["job_id"]
#     print(f"‚úÖ Workflow created! Job ID: {job_id}")
#     print(f"   View at: {host}/jobs/{job_id}")
# else:
#     print(f"‚ùå Error: {response.text}")

# COMMAND ----------

# MAGIC %md
# MAGIC ## 8Ô∏è‚É£ Comparison ‚Äî Orchestration Options
# MAGIC
# MAGIC | Feature | Databricks Workflows | Apache Airflow | dbt Cloud |
# MAGIC |---------|---------------------|----------------|-----------|
# MAGIC | **Native dbt support** | ‚úÖ dbt task type | ‚ö†Ô∏è via BashOperator | ‚úÖ Built-in |
# MAGIC | **Conditional logic** | ‚úÖ IF/ELSE tasks | ‚úÖ BranchOperator | ‚ùå |
# MAGIC | **ForEach loops** | ‚úÖ ForEach task | ‚ö†Ô∏è Dynamic DAGs | ‚ùå |
# MAGIC | **Retry policies** | ‚úÖ Per-task config | ‚úÖ Per-task config | ‚úÖ |
# MAGIC | **Managed service** | ‚úÖ Fully managed | ‚ùå Self-hosted (or MWAA) | ‚úÖ Fully managed |
# MAGIC | **Cost** | Included with Databricks | Separate infra | $100+/month |
# MAGIC | **Delta Lake integration** | ‚úÖ Native | ‚ö†Ô∏è Via provider | ‚ö†Ô∏è Via adapter |
# MAGIC | **Git integration** | ‚úÖ Repos | ‚úÖ DAGs in Git | ‚úÖ Native |
# MAGIC
# MAGIC üí° **Bottom line**: If you're already on Databricks, use **Databricks Workflows** to orchestrate dbt.
# MAGIC It eliminates the need for a separate orchestrator (Airflow, Prefect) and provides native dbt task support.
